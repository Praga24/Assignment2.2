1.HDFS:
 
 Hadoop Distributed File System.
HDFS holds very large amount of data and provides easier access. 
The files are stored across multiple machines. 
These files are stored in redundant fashion to rescue the system from possible data losses in case of failure. 
HDFS also makes applications available to parallel processing.
It consists of the
NameNode-Like a Master. 
DataNode-Like Slaves.
DataNode consists of the actual data and the NameNode manages and coordinates those DataNodes.



2.Hadoop cluster :


A Hadoop cluster is a special type of computational cluster designed specifically for 
storing and analyzing large amounts of unstructured/semi-structured data in a distributed computing environment. 
A hadoop cluster is composed of HDFS and has the NameNode and DataNodes(Master and Slaves). 



3.HDFS Blocks :

• One file can contain many blocks. Since HDFS is designed for large files, block
size is 128 MB by default.
• It gets blocks of local file system continuosly to minimise head seek time.

For ex, Consider a file of size 300MB then,
2 blocks of 128MB each and
1 block of 44MB will be created.